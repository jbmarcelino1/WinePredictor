{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.impute import SimpleImputer\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.preprocessing import RobustScaler  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_raw_data(operating_system ='mac'):\n",
    "    '''generate a dictionary of raw dataframes\n",
    "    \n",
    "    parameters\n",
    "    -----------\n",
    "    type of operating system used windows or mac\n",
    "    default mac\n",
    "    \n",
    "    '''\n",
    "    if operating_system == 'mac':\n",
    "        base_file_path = r\"/Users/{}/Desktop/data\".format(os.getlogin())\n",
    "    if operating_system == 'windows':\n",
    "        base_file_path = r\"C:\\Users\\{}\\Desktop\\data\".format(os.getlogin())\n",
    "    df_dict = dict()\n",
    "    for file in os.listdir(base_file_path):\n",
    "        if file.endswith('.csv'):\n",
    "            df_dict[file.split('.')[0]] = pd.read_csv(os.path.join(base_file_path,file))\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = import_raw_data()\n",
    "df = df_dict.get('winemag-data-130k-v2').copy()\n",
    "temp_df = df_dict.get('temperature').copy()\n",
    "country_iso = df_dict.get('country_iso_data').copy()\n",
    "weather_month_v2 = df_dict.get('weather_country_month_v2').copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    56270\n",
       "3    48608\n",
       "1    12430\n",
       "4    12305\n",
       "5      358\n",
       "Name: test, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changed from using original quantile list, big factor in driving results \n",
    "df['test'] = pd.cut(df['points'],bins=5,labels=[1,2,3,4,5])\n",
    "df['test'].value_counts()\n",
    "sns.countplot(x='test', color='darkgreen',\n",
    "                  data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**columns to remove**\n",
    "\n",
    "description, designation & Twitter handle removed at this stage, explore later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_DROP = ['Unnamed: 0','designation','region_2','taster_twitter_handle','points']\n",
    "df.drop(columns=COLUMN_DROP,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dealing with missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['variety','province','country','taster_name'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country            0\n",
       "description        0\n",
       "price              0\n",
       "province           0\n",
       "region_1       20817\n",
       "taster_name        0\n",
       "title              0\n",
       "variety            0\n",
       "winery             0\n",
       "test               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_median = SimpleImputer(strategy='median')\n",
    "df['price'] = imp_median.fit_transform(df[['price']])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region_1'] = df['region_1'].fillna(\"NONE\")\n",
    "# drop the duplicates in the data\n",
    "df.drop_duplicates(subset=['description','title'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing  & Feature Engineering\n",
    "\n",
    "extracting year from text and cleaing reuslt to produce valid year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## year \n",
    "count_year = df[df['title'].str.contains('\\d',regex=True)].shape[0]\n",
    "df['number_extract'] = df['title'].str.extract('(\\d+)')\n",
    "df['number_extract'] = np.where(len(df['number_extract'])<4 & len(df['number_extract'])<=5,np.nan,df['number_extract'])\n",
    "df['number_extract'] = pd.to_numeric(df['number_extract'])\n",
    "df['number_extract'] = np.where(\n",
    "                    (df['number_extract']>=2021) | (df['number_extract']<=(2021-70)),\n",
    "                    np.nan,\n",
    "                    df['number_extract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding simple imputer but may need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country        0\n",
       "description    0\n",
       "price          0\n",
       "province       0\n",
       "region_1       0\n",
       "taster_name    0\n",
       "title          0\n",
       "variety        0\n",
       "winery         0\n",
       "test           0\n",
       "year           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_median = SimpleImputer(strategy='median')\n",
    "df['number_extract'] = imp_median.fit_transform(df[['number_extract']])\n",
    "df['number_extract'] = pd.to_datetime(df['number_extract'],format='%Y').dt.year \n",
    "df.rename(columns={'number_extract':'year'},inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating length of the name to work out whether name longer names and titles relate to better reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['title','taster_name']:\n",
    "     df[f\"{col}_length\"] = df[col].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change labels to numbers\n",
    "df['price_bin'] = pd.cut(df['price'],bins=15,labels=False)\n",
    "df.drop(columns=['taster_name', 'title'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Weather Data\n",
    "\n",
    "additional feature related to average temperature for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_feature = df.set_index('country').join(country_iso.set_index('country'))\n",
    "weather_iso_df = weather_month_v2.set_index('country').join(country_iso.set_index('country'))\n",
    "weather_iso_df['year'] = pd.to_datetime(weather_iso_df['month']).dt.year\n",
    "weather_iso_summary_df = weather_iso_df.groupby(['country_iso', 'year'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "              weather_feature, \n",
    "              weather_iso_summary_df,  \n",
    "              how='left', \n",
    "              left_on=['country_iso','year'], \n",
    "              right_on=['country_iso','year']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description           0\n",
       "price                 0\n",
       "province              0\n",
       "region_1              0\n",
       "variety               0\n",
       "winery                0\n",
       "test                  0\n",
       "year                  0\n",
       "title_length          0\n",
       "taster_name_length    0\n",
       "price_bin             0\n",
       "country_iso           0\n",
       "avg_temp              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_median = SimpleImputer(strategy='median')\n",
    "df['avg_temp'] = imp_median.fit_transform(df[['avg_temp']])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    return unique_word_length/total_length\n",
    "\n",
    "df['vocab richness'] = df['description'].apply(vocab_richness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/edwardburroughes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# taken from kaggle\n",
    "import string \n",
    "punc = set(string.punctuation)\n",
    "\n",
    "#loading stop_words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# creating a set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# combining the 2 sets with an \"or\" operator (i.e. \"|\")\n",
    "all_stops = stop_words | punc\n",
    "\n",
    "# loop to pre-process data\n",
    "clean_desc =[]\n",
    "for item in df['description'].to_list():\n",
    "    tok_desc = word_tokenize(item)\n",
    "    lower_data = [i.lower() for i in tok_desc]\n",
    "    tok_desc_no_num = [i for i in lower_data if i.isalpha()]\n",
    "    filtered_desc = [i for i in tok_desc_no_num if i not in all_stops]\n",
    "    clean_desc.append(filtered_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing the data in a new dataframe\n",
    "clean_desc_untok = [' '.join(i) for i in clean_desc]\n",
    "column_names = ['original_desc', 'untok_description']\n",
    "data_tuple= list(zip(df['description'], clean_desc_untok))\n",
    "desc_df = pd.DataFrame(data_tuple, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/edwardburroughes/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "tb = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "blob = [tb(text) for text in desc_df['untok_description']]\n",
    "sentiment_values = [text.sentiment for text in blob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>0.0774711</td>\n",
       "      <td>0.922529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>0.835127</td>\n",
       "      <td>0.164873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>0.995154</td>\n",
       "      <td>0.00484628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>0.0718264</td>\n",
       "      <td>0.928174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>0.947882</td>\n",
       "      <td>0.0521175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clf        pos         neg\n",
       "0  neg  0.0774711    0.922529\n",
       "1  pos   0.835127    0.164873\n",
       "2  pos   0.995154  0.00484628\n",
       "3  neg  0.0718264    0.928174\n",
       "4  pos   0.947882   0.0521175"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pd.DataFrame(zip(*sentiment_values)).T\n",
    "stats.columns = ['clf','pos','neg']\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(stats['clf'])\n",
    "df = df.join(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move back onto original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['test','description','clf'])\n",
    "y = df[['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature selection defining best & worst features for one hot encoded categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1(model,X_test,y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = f1_score(y_test,y_pred,average='weighted')\n",
    "    return score\n",
    "\n",
    "def create_dummies_ohe(X,cat_columns):\n",
    "    categorical_x = pd.get_dummies(X[cat_columns],prefix='cat')\n",
    "    numeric_cols = list(set(X.columns)-set(cat_columns))\n",
    "    return pd.concat([X[numeric_cols],categorical_x], axis=1)\n",
    "\n",
    "def create_dummies_le(X,cat_columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in cat_columns:\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout method on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the features selected above\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "features = pd.read_csv(r\"/Users/edwardburroughes/Desktop/data_test.csv\")\n",
    "numeric_features = ['price','title_length','taster_name_length','avg_temp','year','pos','neg','vocab richness']\n",
    "cat_features = ['province','variety','country_iso','price_bin','winery','region_1']\n",
    "\n",
    "def select_cat_data_threshold(X,feature_data,threshold_value,cat_features=cat_features):\n",
    "    dictionary_filter = {cat:feature_data.loc[(feature_data[cat]==True) & \n",
    "                                              (feature_data['scores']<=threshold_value),'features']for cat in cat_features}\n",
    "    for cat,series in dictionary_filter.items():\n",
    "        X.loc[X[cat].isin(series),cat] = 'Other'\n",
    "    return (X,dictionary_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test whether the model speed and F1 given the threshold defined above and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6051847601369508\n",
      "0:42:57.619901\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# start = datetime.datetime.now()\n",
    "# X,features_replace = select_cat_data_threshold(X,features,1E-6)\n",
    "# X = create_dummies_ohe(X,cat_features)\n",
    "# X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "# sc = QuantileTransformer()\n",
    "# X_train[numeric_features] = sc.fit_transform(X_train[numeric_features])\n",
    "# X_test[numeric_features] = sc.fit_transform(X_test[numeric_features])\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "# rf_classifier.fit(X_train, y_train)\n",
    "# y_pred = rf_classifier.predict(X_test)\n",
    "# print(f1_score(y_test,y_pred,average='weighted'))\n",
    "# end = datetime.datetime.now()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "X,features_replace = select_cat_data_threshold(X,features,1E-6)\n",
    "X = create_dummies_ohe(X,cat_features)\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "sc = QuantileTransformer()\n",
    "X_train[numeric_features] = sc.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = sc.fit_transform(X_test[numeric_features])\n",
    "xgb_classifier = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=7, n_estimators=200))\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(f1_score(y_test,y_pred,average='weighted'))\n",
    "end = datetime.datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using F1-score as imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using xgboost is almost is definitely better still very low lets amend the imbalances in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,SVMSMOTE\n",
    "from sklearn import neighbors\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "over = SMOTE()\n",
    "under = RandomUnderSampler()\n",
    "steps = [('o', over), ('u', under)]\n",
    "smote_pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample = BorderlineSMOTE(sampling_strategy='minority',k_neighbors=1,m_neighbors=1)\n",
    "X_train_resample,y_train_resample = over.fit_resample(X_train,y_train)\n",
    "xgb_classifier.fit(X_train_resample,y_train_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5953507613327068"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(rf_classifier,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smote perhaps a more focused sampling strategy is required for balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('o',\n",
       "                                        BorderlineSMOTE(k_neighbors=1,\n",
       "                                                        m_neighbors=1,\n",
       "                                                        sampling_strategy='minority')),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'o__k_neighbors': [1, 5], 'o__m_neighbors': [1, 20],\n",
       "                         'o__sampling_strategy': ('minority', 'not majority',\n",
       "                                                  'all')},\n",
       "             scoring='f1_weighted', verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "pipeline = Pipeline([('o', oversample),\n",
    "           ('rf', RandomForestClassifier())])\n",
    "parameters = {'o__sampling_strategy':('minority','not majority','all'),\n",
    "             'o__k_neighbors':[1,5],\n",
    "             'o__m_neighbors':[1,20]}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"f1_weighted\", \n",
    "                           refit=True, cv=5)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o__k_neighbors': 1, 'o__m_neighbors': 20, 'o__sampling_strategy': 'minority'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {\"learning_rate\":[0.05,0.1,0.15,0.2,0.25,0.3,0.35],\n",
    "          \"max_depth\":[3,4,5,6,8,10,12,15,17],\n",
    "          \"min_child_weight\":[1,3,5,7,9,11],\n",
    "          \"gamma\":[0.0,0.1,0.3,0.4,0.5,0.6],\n",
    "          \"colsample_bytree\":[0.3,0.4,0.5,0.7,0.8,0.9],\n",
    "          \"n_estimators\":range(60, 220, 40)\n",
    "         }\n",
    "xgb_random_search = RandomizedSearchCV(xgb,\n",
    "                                       param_distributions=params,\n",
    "                                       cv=5,\n",
    "                                       scoring='f1_weighted',\n",
    "                                       verbose=3,\n",
    "                                       n_iter = 10,\n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=15, min_child_weight=1, n_estimators=180; total time=  35.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=15, min_child_weight=1, n_estimators=180; total time=  37.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=15, min_child_weight=1, n_estimators=180; total time=  37.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=15, min_child_weight=1, n_estimators=180; total time=  40.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=15, min_child_weight=1, n_estimators=180; total time=  37.4s\n",
      "[CV 1/5] END colsample_bytree=0.4, gamma=0.3, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=60; total time=   3.7s\n",
      "[CV 2/5] END colsample_bytree=0.4, gamma=0.3, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=60; total time=   3.7s\n",
      "[CV 3/5] END colsample_bytree=0.4, gamma=0.3, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=60; total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=0.4, gamma=0.3, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=60; total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.4, gamma=0.3, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=60; total time=   3.8s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.4, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=60; total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.4, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=60; total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.4, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=60; total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.4, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=60; total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.4, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=60; total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.2, max_depth=8, min_child_weight=7, n_estimators=180; total time=  20.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.2, max_depth=8, min_child_weight=7, n_estimators=180; total time=  21.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.2, max_depth=8, min_child_weight=7, n_estimators=180; total time=  21.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.2, max_depth=8, min_child_weight=7, n_estimators=180; total time=  20.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.2, max_depth=8, min_child_weight=7, n_estimators=180; total time=  20.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.05, max_depth=12, min_child_weight=9, n_estimators=100; total time=  18.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.05, max_depth=12, min_child_weight=9, n_estimators=100; total time=  19.0s\n",
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.05, max_depth=12, min_child_weight=9, n_estimators=100; total time=  18.7s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.05, max_depth=12, min_child_weight=9, n_estimators=100; total time=  18.6s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.05, max_depth=12, min_child_weight=9, n_estimators=100; total time=  18.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.35, max_depth=6, min_child_weight=9, n_estimators=180; total time=  16.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.35, max_depth=6, min_child_weight=9, n_estimators=180; total time=  17.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.35, max_depth=6, min_child_weight=9, n_estimators=180; total time=  16.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.35, max_depth=6, min_child_weight=9, n_estimators=180; total time=  16.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.35, max_depth=6, min_child_weight=9, n_estimators=180; total time=  16.6s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.6, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100; total time=   3.8s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.6, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100; total time=   3.9s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.6, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100; total time=   4.0s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.6, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100; total time=   3.9s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.6, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100; total time=   3.8s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.15, max_depth=17, min_child_weight=7, n_estimators=180; total time=  44.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.15, max_depth=17, min_child_weight=7, n_estimators=180; total time=  45.5s\n",
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.15, max_depth=17, min_child_weight=7, n_estimators=180; total time=  43.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.15, max_depth=17, min_child_weight=7, n_estimators=180; total time=  43.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.15, max_depth=17, min_child_weight=7, n_estimators=180; total time=  45.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.3, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=60; total time=   4.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.3, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=60; total time=   3.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.3, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=60; total time=   3.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.3, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=60; total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.3, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=60; total time=   3.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=6, min_child_weight=5, n_estimators=60; total time=   5.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=6, min_child_weight=5, n_estimators=60; total time=   5.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=6, min_child_weight=5, n_estimators=60; total time=   5.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=6, min_child_weight=5, n_estimators=60; total time=   5.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.4, learning_rate=0.3, max_depth=6, min_child_weight=5, n_estimators=60; total time=   5.3s\n"
     ]
    }
   ],
   "source": [
    "xgb_params = xgb_random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model best score 0.6067761324275728\n",
      "model best params {'n_estimators': 180, 'min_child_weight': 7, 'max_depth': 8, 'learning_rate': 0.2, 'gamma': 0.4, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "print(f\"model best score {xgb_params.best_score_}\")\n",
    "print(f\"model best params {xgb_params.best_params_}\")\n",
    "params = xgb_params.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
